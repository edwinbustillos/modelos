# Modelos de IA

Este repositÃ³rio contÃ©m modelos de inteligÃªncia artificial para uso local.

## ğŸ“¥ Como fazer download

### PrÃ©-requisitos

Para trabalhar com este repositÃ³rio que usa Git LFS (Large File Storage), vocÃª precisarÃ¡:

1. **Git** instalado em seu sistema
2. **Git LFS** instalado e configurado

### InstalaÃ§Ã£o do Git LFS

Se vocÃª ainda nÃ£o tem o Git LFS instalado:

```bash
# No macOS (usando Homebrew)
brew install git-lfs

# No Ubuntu/Debian
sudo apt install git-lfs

# No Windows
# Baixe de: https://git-lfs.github.io/
```

### Download do repositÃ³rio

1. **Clone o repositÃ³rio:**
   ```bash
   git clone https://github.com/edwinbustillos/modelos.git
   cd modelos
   ```

2. **Inicialize o Git LFS (se necessÃ¡rio):**
   ```bash
   git lfs install
   ```

3. **Baixe os arquivos grandes:**
   ```bash
   git lfs pull
   ```

## ğŸ“ Estrutura do Projeto

```
modelos/
â”œâ”€â”€ ğŸ“„ README.md                    # Este arquivo
â”œâ”€â”€ ğŸ¤– llama3-small-Q3_K_S.gguf   # Modelo Llama 3 Small (104.33 MB)
â”œâ”€â”€ ğŸ³ Dockerfile                   # ConfiguraÃ§Ã£o Docker
â”œâ”€â”€ ğŸ™ docker-compose.yml          # OrquestraÃ§Ã£o de serviÃ§os
â”œâ”€â”€ ğŸš€ ollama.sh                   # Script unificado (tudo em um!)
â””â”€â”€ ğŸ“ models/                     # DiretÃ³rio para modelos (volume Docker)
```

## ğŸ”§ Uso

### OpÃ§Ã£o 1: AI CLI (Novo! â­)

A maneira mais moderna de interagir com seus modelos locais - um CLI similar ao Claude Code:

```bash
# InstalaÃ§Ã£o rÃ¡pida
./install-ai-cli.sh

# Uso bÃ¡sico
./ai-cli chat "Hello, how are you?"
./ai-cli code "Create a Python sorting function"  
./ai-cli explain "What is machine learning?"
./ai-cli translate "Hello world" --to portuguese
./ai-cli review mycode.py

# Chat interativo
./ai-cli chat --interactive

# Com modelo especÃ­fico
./ai-cli --model llama3.2 chat "Explain quantum physics"
```

**âœ¨ Funcionalidades do AI CLI:**
- ğŸ’¬ **Chat interativo** e por comando
- ğŸ–¥ï¸ **GeraÃ§Ã£o de cÃ³digo** com exemplos
- ğŸ“š **ExplicaÃ§Ãµes detalhadas** de conceitos
- ğŸŒ **TraduÃ§Ã£o** para qualquer idioma
- ğŸ“„ **Resumos** de textos e arquivos
- ğŸ” **Review automÃ¡tico** de cÃ³digo
- ğŸ¨ **Interface colorida** e amigÃ¡vel

### OpÃ§Ã£o 2: Docker + WebUI

A maneira mais fÃ¡cil de usar os modelos Ã© com Docker + Ollama + Interface Web:

```bash
# Clone o repositÃ³rio
git clone https://github.com/edwinbustillos/modelos.git
cd modelos

# Execute o setup automÃ¡tico
./ollama.sh start
```

Isso irÃ¡:
- ğŸ³ Construir e iniciar containers Docker
- ğŸ¤– Configurar Ollama com os modelos GGUF  
- ğŸŒ Disponibilizar interface web em http://localhost:3000
- ğŸ“¡ API Ollama em http://localhost:11434

**Comandos Ãºteis:**
```bash
./ollama.sh start    # Iniciar tudo
./ollama.sh stop     # Parar serviÃ§os
./ollama.sh restart  # Reiniciar serviÃ§os
./ollama.sh import   # Importar modelos GGUF
./ollama.sh status   # Verificar status
./ollama.sh help     # Ajuda completa
```

### OpÃ§Ã£o 3: Uso Direto

Os arquivos `.gguf` sÃ£o modelos quantizados que podem ser usados com:
- **llama.cpp**
- **Ollama**
- **LM Studio** 
- **GPT4All**
- Outras ferramentas compatÃ­veis com o formato GGUF

## âš ï¸ ObservaÃ§Ãµes importantes

- Este repositÃ³rio usa Git LFS para gerenciar arquivos grandes
- Certifique-se de ter o Git LFS instalado antes de clonar
- O download pode demorar dependendo da sua conexÃ£o (arquivo de ~104 MB)
- Os modelos sÃ£o fornecidos "como estÃ£o" para fins de pesquisa e desenvolvimento

## ğŸ“„ LicenÃ§a

Consulte a documentaÃ§Ã£o original dos modelos para informaÃ§Ãµes sobre licenciamento.